{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a66a3d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    \n",
    "<br>\n",
    "\n",
    "# FINE-GRAINED EMOTIONAL CONTROL OF TEXT-TO-SPEECH \n",
    "\n",
    "### LEARNING TO RANK INTER- AND INTRA-CLASS EMOTION INTENSITIES\n",
    "\n",
    "Shijun Wang, Jón Guðnason, Damian Borth\n",
    "\n",
    "**ICASSP 2023**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "694aa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. Paths\n",
    "##############################################\n",
    "DATA_PATH           = '/workspace/data/EmoV-DB'\n",
    "CORPUS_PATH         = '/workspace/montreal_forced_aligner/corpus'\n",
    "TEXTGRID_PATH       = '/workspace/montreal_forced_aligner/aligned'\n",
    "PREPROCESSED_PATH   = '/workspace/preprocessed'\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 2. Preprocessing\n",
    "##############################################\n",
    "NOISE_SYMBOL        = ' [noise] '\n",
    "SPEAKERS            = ['bea', 'jenie', 'josh', 'sam']\n",
    "EMOTIONS            = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n",
    "SIL_PHONES          = ['sil', 'spn', 'sp', '']\n",
    "PITCH_AVERAGING     = True\n",
    "ENERGY_AVERAGING    = True\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 3. Audio (optimized for vocoder)\n",
    "##############################################\n",
    "SAMPLING_RATE       = 16000\n",
    "HOP_LENGTH          = 256\n",
    "WIN_LENGTH          = 1024\n",
    "N_FFT               = 1024\n",
    "N_MELS              = 80\n",
    "F_MIN               = 0.0\n",
    "F_MAX               = 8000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "511b3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tgt\n",
    "import glob\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import scipy\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pyworld as pw\n",
    "\n",
    "from text import _clean_text\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from speechbrain.lobes.models.FastSpeech2 import mel_spectogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b9c51",
   "metadata": {},
   "source": [
    "**1. Preprocessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/emo_rank_tts/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
      "/workspace/emo_rank_tts/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
     ]
    }
   ],
   "source": [
    "audio_id_to_transcript = {}\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'cmuarctic.data')) as f:\n",
    "    for line in f.readlines():\n",
    "        audio_id, transcript = line[2:-2].split('\\\"')[:2]\n",
    "\n",
    "        audio_id = audio_id.strip()\n",
    "        transcript = transcript.strip()\n",
    "\n",
    "        if audio_id.startswith('arctic_b'):\n",
    "            continue\n",
    "        \n",
    "        audio_id = audio_id[-4:]\n",
    "        transcript = NOISE_SYMBOL + _clean_text(transcript, ['english_cleaners']) + NOISE_SYMBOL\n",
    "\n",
    "        audio_id_to_transcript[audio_id] = transcript.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:39<00:00, 39.76s/it]\n"
     ]
    }
   ],
   "source": [
    "for speaker in tqdm.tqdm(SPEAKERS):\n",
    "    for emotion in EMOTIONS:\n",
    "\n",
    "        # check the path existence: josh has only three emotions\n",
    "        spk_emo_path = os.path.join(DATA_PATH, speaker, emotion)\n",
    "        if not os.path.exists(spk_emo_path):\n",
    "            continue\n",
    "        \n",
    "        # resample and create .lab file\n",
    "        for wav_path in glob.glob(os.path.join(spk_emo_path, '*.wav')):\n",
    "\n",
    "            y, sr = librosa.load(wav_path, sr=SAMPLING_RATE)\n",
    "\n",
    "            audio_id = os.path.basename(wav_path)[:-4]\n",
    "            transcript = audio_id_to_transcript[audio_id]\n",
    "\n",
    "            os.makedirs(os.path.join(CORPUS_PATH, speaker), exist_ok=True)\n",
    "\n",
    "            tgt_path = os.path.join(CORPUS_PATH, speaker, f'{emotion}_{audio_id}')\n",
    "            scipy.io.wavfile.write(tgt_path + '.wav', sr, y)\n",
    "\n",
    "            with open(tgt_path + '.lab', 'w') as f:\n",
    "                f.write(transcript + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f9864",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# download speech dictionary\n",
    "wget -O /workspace/montreal_forced_aligner/librispeech-lexicon.txt https://openslr.trmal.net/resources/11/librispeech-lexicon.txt \n",
    "\n",
    "# prepare environment for montreal forced aligner\n",
    "conda create -n aligner -c conda-forge montreal-forced-aligner -y\n",
    "\n",
    "# **important** please make sure to select `aligner` environment\n",
    "mfa model download acoustic english_us_arpa\n",
    "mfa validate /workspace/montreal_forced_aligner/corpus /workspace/montreal_forced_aligner/librispeech-lexicon.txt english_us_arpa\n",
    "mfa align /workspace/montreal_forced_aligner/corpus /workspace/montreal_forced_aligner/librispeech-lexicon.txt english_us_arpa /workspace/montreal_forced_aligner/aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "538dd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_textgrid(textgrid_file):\n",
    "\n",
    "    phones = []\n",
    "    durations = []\n",
    "    speech_start_time = 0.0\n",
    "    speech_end_time = 0.0\n",
    "    end_idx = 0\n",
    "\n",
    "    tg = tgt.io.read_textgrid(textgrid_file, include_empty_intervals=True)\n",
    "    tier = tg.get_tier_by_name('phones')\n",
    "\n",
    "    for t in tier._objects:\n",
    "        \n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "\n",
    "        if len(phones) == 0:\n",
    "            if p in SIL_PHONES:\n",
    "                continue\n",
    "            speech_start_time = s\n",
    "\n",
    "        if p not in SIL_PHONES:\n",
    "            phones.append(p)\n",
    "            speech_end_time = e\n",
    "            end_idx = len(phones)\n",
    "        else:\n",
    "            phones.append('spn')\n",
    "\n",
    "        durations.append(\n",
    "            int(\n",
    "                np.round(e * SAMPLING_RATE / HOP_LENGTH) -\n",
    "                np.round(s * SAMPLING_RATE / HOP_LENGTH)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    phones = phones[:end_idx]\n",
    "    durations = durations[:end_idx]\n",
    "\n",
    "    return phones, durations, speech_start_time, speech_end_time\n",
    "\n",
    "\n",
    "def trim_audio(y, start_time, end_time):\n",
    "    start_idx = int(np.round(start_time * SAMPLING_RATE))\n",
    "    end_idx = int(np.round(end_time * SAMPLING_RATE))\n",
    "\n",
    "    y = y[start_idx:end_idx].astype(np.float32)\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_pitch(y):\n",
    "    y = y.astype(np.float64)\n",
    "    f0, t = pw.dio(y, SAMPLING_RATE, frame_period=HOP_LENGTH / SAMPLING_RATE * 1000)\n",
    "    f0 = pw.stonemask(y, f0, t, SAMPLING_RATE)\n",
    "   \n",
    "    return f0\n",
    "\n",
    "\n",
    "def get_mel(y):\n",
    "    y = torch.FloatTensor(y)\n",
    "    mel, energy = mel_spectogram(\n",
    "        audio=y,\n",
    "        sample_rate=SAMPLING_RATE,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=WIN_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        n_fft=N_FFT,\n",
    "        f_min=F_MIN,\n",
    "        f_max=F_MAX,\n",
    "        power=1,\n",
    "        normalized=False,\n",
    "        min_max_energy_norm=True,\n",
    "        norm=\"slaney\",\n",
    "        mel_scale=\"slaney\",\n",
    "        compression=True\n",
    "    )\n",
    "    return mel, energy\n",
    "\n",
    "\n",
    "def expand(values, durations):\n",
    "    out = list()\n",
    "    for value, d in zip(values, durations):\n",
    "        out += [value] * max(0, int(d))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "102fdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(speaker, emotion):\n",
    "\n",
    "    def _remove_outliers(x):\n",
    "        p25 = np.percentile(x, 25)\n",
    "        p75 = np.percentile(x, 75)\n",
    "        lower = p25 - 1.5 * (p75 - p25)\n",
    "        upper = p75 + 1.5 * (p75 - p25)\n",
    "        normal_indices = np.logical_and(x >= lower, x <= upper)\n",
    "        return x[normal_indices]\n",
    "\n",
    "    def _normalize(name, mean, std):\n",
    "        \n",
    "        max_value = np.finfo(np.float64).min\n",
    "        min_value = np.finfo(np.float64).max\n",
    "\n",
    "        for preprocessed_path in glob.glob(os.path.join(PREPROCESSED_PATH, speaker, f'{emotion}_*.npz')):\n",
    "            data = dict(np.load(preprocessed_path))\n",
    "\n",
    "            data[name] = (data[name] - mean) / std\n",
    "            np.savez(preprocessed_path, **data)\n",
    "\n",
    "            max_value = max(max_value, max(data[name]))\n",
    "            min_value = min(min_value, min(data[name]))\n",
    "\n",
    "        return min_value, max_value\n",
    "\n",
    "    pitch_scaler = StandardScaler()\n",
    "    energy_scaler = StandardScaler()\n",
    "\n",
    "    for audio_path in glob.glob(os.path.join(CORPUS_PATH, speaker, f'{emotion}_*.wav')):\n",
    "\n",
    "        audio_id = os.path.basename(audio_path)[:-4].split('_')[-1]\n",
    "        \n",
    "        textgrid_path = os.path.join(TEXTGRID_PATH, speaker, f'{emotion}_{audio_id}.TextGrid')\n",
    "        transcript_path = os.path.join(CORPUS_PATH, speaker, f'{emotion}_{audio_id}.lab')\n",
    "\n",
    "        # check the path existence\n",
    "        if not os.path.exists(textgrid_path):\n",
    "            continue\n",
    "\n",
    "        phones, durations, speech_start_time, speech_end_time = process_textgrid(textgrid_path)\n",
    "\n",
    "        if speech_start_time >= speech_end_time:\n",
    "            print(f\"Invalid start/end: {audio_path}\")\n",
    "\n",
    "        # trim audio\n",
    "        y, sr = librosa.load(audio_path, sr=SAMPLING_RATE)\n",
    "        y = trim_audio(y, speech_start_time, speech_end_time)\n",
    "\n",
    "        # transcript\n",
    "        with open(transcript_path, 'r') as f:\n",
    "            transcript = f.read().strip().replace(NOISE_SYMBOL, '')\n",
    "\n",
    "        # pitch\n",
    "        pitch = get_pitch(y)\n",
    "        if np.sum(pitch != 0) <= 1:\n",
    "            print(f\"Invalid pitch: {audio_path}\")\n",
    "            continue\n",
    "        pitch = pitch[:sum(durations)]\n",
    "        \n",
    "        # melspectrogram, energy\n",
    "        mel, energy = get_mel(y)\n",
    "        mel = mel.numpy()\n",
    "        energy = energy.numpy()\n",
    "        mel = mel[:, :sum(durations)]\n",
    "        energy = energy[:sum(durations)]\n",
    "\n",
    "\n",
    "        if PITCH_AVERAGING:\n",
    "            nonzero_ids = np.where(pitch != 0)[0]\n",
    "            interp_fn = scipy.interpolate.interp1d(\n",
    "                nonzero_ids,\n",
    "                pitch[nonzero_ids],\n",
    "                kind='linear',\n",
    "                fill_value=(pitch[nonzero_ids[0]], pitch[nonzero_ids[-1]]),\n",
    "                bounds_error=False  \n",
    "            )\n",
    "            pitch = interp_fn(np.arange(len(pitch)))\n",
    "\n",
    "            pos = 0\n",
    "            for i, d in enumerate(durations):\n",
    "                if d > 0:\n",
    "                    pitch[i] = np.mean(pitch[pos:pos + d])\n",
    "                else:\n",
    "                    pitch[i] = 0\n",
    "                pos += d\n",
    "            pitch = pitch[:len(durations)]\n",
    "\n",
    "        if ENERGY_AVERAGING:\n",
    "            pos = 0\n",
    "            for i, d in enumerate(durations):\n",
    "                if d > 0:\n",
    "                    energy[i] = np.mean(energy[pos:pos + d])\n",
    "                else:\n",
    "                    energy[i] = 0\n",
    "                pos += d\n",
    "            energy = energy[:len(durations)]\n",
    "\n",
    "        \n",
    "        # remove outliers\n",
    "        outlier_removed_pitch = _remove_outliers(pitch)\n",
    "        outlier_removed_energy = _remove_outliers(energy)\n",
    "\n",
    "        pitch_scaler.partial_fit(outlier_removed_pitch.reshape((-1, 1)))\n",
    "        energy_scaler.partial_fit(outlier_removed_energy.reshape((-1, 1)))\n",
    "\n",
    "\n",
    "        # save artifacts\n",
    "        np.savez(\n",
    "            os.path.join(PREPROCESSED_PATH, speaker, f'{emotion}_{audio_id}.npz'),\n",
    "            \n",
    "            # metadata\n",
    "            phones=phones,\n",
    "            emotion=emotion,\n",
    "            speaker=speaker,\n",
    "            audio_id=audio_id,\n",
    "            audio_path=audio_path,\n",
    "            transcript=transcript,\n",
    "            textgrid_path=textgrid_path,\n",
    "            \n",
    "            # inputs\n",
    "            mel=mel,\n",
    "            pitch=pitch,\n",
    "            energy=energy,\n",
    "            durations=durations,\n",
    "        )\n",
    "\n",
    "    pitch_mean, pitch_std = pitch_scaler.mean_[0], pitch_scaler.scale_[0]\n",
    "    energy_mean, energy_std = energy_scaler.mean_[0], energy_scaler.scale_[0]\n",
    "\n",
    "    print(\"* Calculating statistics for pitch and energy\")\n",
    "    pitch_min, pitch_max = _normalize('pitch', pitch_mean, pitch_std)\n",
    "    energy_min, energy_max = _normalize('energy', energy_mean, energy_std)\n",
    "\n",
    "    if os.path.exists(os.path.join(PREPROCESSED_PATH, 'stats.json')):\n",
    "        with open(os.path.join(PREPROCESSED_PATH, 'stats.json'), 'r') as f:\n",
    "            stats = json.load(f)\n",
    "    else:\n",
    "        stats = {}\n",
    "    \n",
    "    with open(os.path.join(PREPROCESSED_PATH, 'stats.json'), 'w') as f:\n",
    "        new_stats = {\n",
    "            'pitch': [float(pitch_min), float(pitch_max), float(pitch_mean), float(pitch_std)],\n",
    "            'energy': [float(energy_min), float(energy_max), float(energy_mean), float(energy_std)],\n",
    "        }\n",
    "        stats[speaker] = {}\n",
    "        stats[speaker][emotion] = new_stats\n",
    "        json.dump(stats, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ad4cb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bea neutral:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bea amused:   0%|          | 0/4 [00:18<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bea angry:   0%|          | 0/4 [00:33<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bea disgusted:   0%|          | 0/4 [00:49<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bea sleepy:   0%|          | 0/4 [01:08<?, ?it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jenie neutral:  25%|██▌       | 1/4 [01:37<04:51, 97.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jenie amused:  25%|██▌       | 1/4 [01:58<04:51, 97.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jenie angry:  25%|██▌       | 1/4 [02:10<04:51, 97.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jenie disgusted:  25%|██▌       | 1/4 [02:38<04:51, 97.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jenie sleepy:  25%|██▌       | 1/4 [02:48<04:51, 97.12s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "josh neutral:  50%|█████     | 2/4 [03:14<03:14, 97.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "josh amused:  50%|█████     | 2/4 [03:29<03:14, 97.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "josh sleepy:  50%|█████     | 2/4 [03:49<03:14, 97.19s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pitch: /workspace/montreal_forced_aligner/corpus/josh/sleepy_0054.wav\n",
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam neutral:  75%|███████▌  | 3/4 [04:07<01:16, 76.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam amused:  75%|███████▌  | 3/4 [04:40<01:16, 76.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam angry:  75%|███████▌  | 3/4 [05:17<01:16, 76.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam disgusted:  75%|███████▌  | 3/4 [05:48<01:16, 76.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam sleepy:  75%|███████▌  | 3/4 [06:24<01:16, 76.88s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Calculating statistics for pitch and energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sam sleepy: 100%|██████████| 4/4 [07:00<00:00, 105.24s/it]\n"
     ]
    }
   ],
   "source": [
    "tbar = tqdm.tqdm(SPEAKERS)\n",
    "for speaker in tbar:\n",
    "\n",
    "    for emotion in EMOTIONS:\n",
    "        \n",
    "        tbar.set_description(f'{speaker} {emotion}')\n",
    "\n",
    "        # check the path existence\n",
    "        if not os.path.exists(os.path.join(DATA_PATH, speaker, emotion)):\n",
    "            continue\n",
    "\n",
    "        # preprocessed path\n",
    "        os.makedirs(os.path.join(PREPROCESSED_PATH, speaker), exist_ok=True)\n",
    "\n",
    "        # mel, energy, pitch, durations\n",
    "        feature_extraction(speaker, emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "from speechbrain.lobes.models.FastSpeech2 import mel_spectogram\n",
    "\n",
    "# Load a pretrained HIFIGAN Vocoder\n",
    "hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-libritts-16kHz\", savedir=\"pretrained_models/tts-hifigan-libritts-16kHz\")\n",
    "\n",
    "# Load an audio file (an example file can be found in this repository)\n",
    "# Ensure that the audio signal is sampled at 16000 Hz; refer to the provided link for a 22050 Hz Vocoder.\n",
    "signal, rate = torchaudio.load('/workspace/montreal_forced_aligner/corpus/bea/amused_0001.wav')\n",
    "\n",
    "# Ensure the audio is sigle channel\n",
    "signal = signal[0].squeeze()\n",
    "\n",
    "torchaudio.save('/workspace/waveform.wav', signal.unsqueeze(0), 16000)\n",
    "\n",
    "# Compute the mel spectrogram.\n",
    "# IMPORTANT: Use these specific parameters to match the Vocoder's training settings for optimal results.\n",
    "spectrogram, _ = mel_spectogram(\n",
    "    audio=signal.squeeze(),\n",
    "    sample_rate=SAM,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    n_fft=1024,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1,\n",
    "    normalized=False,\n",
    "    min_max_energy_norm=True,\n",
    "    norm=\"slaney\",\n",
    "    mel_scale=\"slaney\",\n",
    "    compression=True\n",
    ")\n",
    "\n",
    "# Convert the spectrogram to waveform\n",
    "waveforms = hifi_gan.decode_batch(spectrogram)\n",
    "\n",
    "# Save the reconstructed audio as a waveform\n",
    "torchaudio.save('/workspace/waveform_reconstructed.wav', waveforms.squeeze(1), 16000)\n",
    "\n",
    "# If everything is set up correctly, the original and reconstructed audio should be nearly indistinguishable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
