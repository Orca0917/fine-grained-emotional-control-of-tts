{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a66a3d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    \n",
    "<br>\n",
    "\n",
    "## FINE-GRAINED EMOTIONAL CONTROL OF TEXT-TO-SPEECH\n",
    "\n",
    "#### LEARNING TO RANK INTER- AND INTRA-CLASS EMOTION INTENSITIES\n",
    "\n",
    "Shijun Wang, Jón Guðnason, Damian Borth\n",
    "\n",
    "**ICASSP 2023**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b9c51",
   "metadata": {},
   "source": [
    "**1. Preprocessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f23f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from text import _clean_text\n",
    "\n",
    "DATA_PATH = '/workspace/data/EmoV-DB'\n",
    "TARGET_DATA_PATH = '/workspace/montreal_forced_aligner/corpus'\n",
    "NOISE_SYMBOL = ' [noise] '\n",
    "\n",
    "audio_id_to_transcript = {}\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'cmuarctic.data')) as f:\n",
    "    for line in f.readlines():\n",
    "        audio_id, transcript = line[2:-2].split('\\\"')[:2]\n",
    "\n",
    "        audio_id = audio_id.strip()\n",
    "        transcript = transcript.strip()\n",
    "\n",
    "        if audio_id.startswith('arctic_b'):\n",
    "            continue\n",
    "        \n",
    "        audio_id = audio_id[-4:]\n",
    "        transcript = NOISE_SYMBOL + _clean_text(transcript, ['english_cleaners']) + NOISE_SYMBOL\n",
    "\n",
    "        audio_id_to_transcript[audio_id] = transcript.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fa8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.77s/it]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import scipy\n",
    "import librosa\n",
    "\n",
    "speakers = ['bea', 'jenie', 'josh', 'sam']\n",
    "emotions = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n",
    "\n",
    "for speaker in tqdm.tqdm(speakers):\n",
    "    for emotion in emotions:\n",
    "\n",
    "        # check the path existence: josh has only three emotions\n",
    "        spk_emo_path = os.path.join(DATA_PATH, speaker, emotion)\n",
    "        if not os.path.exists(spk_emo_path):\n",
    "            continue\n",
    "        \n",
    "        # resample and create .lab file\n",
    "        for wav_path in glob.glob(os.path.join(spk_emo_path, '*.wav')):\n",
    "\n",
    "            y, sr = librosa.load(wav_path, sr=16000)\n",
    "\n",
    "            audio_id = wav_path[-8:-4]\n",
    "            transcript = audio_id_to_transcript[audio_id]\n",
    "\n",
    "            os.makedirs(os.path.join(TARGET_DATA_PATH, speaker), exist_ok=True)\n",
    "\n",
    "            tgt_path = os.path.join(TARGET_DATA_PATH, speaker, f'{emotion}_{audio_id}')\n",
    "            scipy.io.wavfile.write(tgt_path + '.wav', sr, y)\n",
    "\n",
    "            with open(tgt_path + '.lab', 'w') as f:\n",
    "                f.write(transcript + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80de427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
