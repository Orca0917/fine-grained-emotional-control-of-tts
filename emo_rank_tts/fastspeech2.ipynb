{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f159fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tgt\n",
    "import glob\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import scipy\n",
    "import random\n",
    "import librosa\n",
    "import sklearn\n",
    "import speechbrain\n",
    "import numpy as np\n",
    "import pyworld as pw\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from text import _clean_text\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "from matplotlib.lines import Line2D\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from speechbrain.lobes.models.FastSpeech2 import mel_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a336ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. Paths\n",
    "##############################################\n",
    "DATA_PATH           = '/workspace/data/EmoV-DB'\n",
    "CORPUS_PATH         = '/workspace/montreal_forced_aligner/corpus'\n",
    "TEXTGRID_PATH       = '/workspace/montreal_forced_aligner/aligned'\n",
    "PREPROCESSED_PATH   = '/workspace/preprocessed'\n",
    "EXPERIMENT_PATH     = '/workspace/experiments'\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 2. Preprocessing\n",
    "##############################################\n",
    "NOISE_SYMBOL        = ' [noise] '\n",
    "SPEAKERS            = ['bea', 'jenie', 'josh', 'sam']\n",
    "EMOTIONS            = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n",
    "SIL_PHONES          = ['sil', 'spn', 'sp', '']\n",
    "VALID_TOKENS        = ['@'] + speechbrain.utils.text_to_sequence.valid_symbols + SIL_PHONES\n",
    "PITCH_AVERAGING     = False\n",
    "ENERGY_AVERAGING    = False\n",
    "MATCH_TRANSCRIPT    = True\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 3. Audio (optimized for vocoder)\n",
    "##############################################\n",
    "SAMPLING_RATE       = 16000\n",
    "HOP_LENGTH          = 256\n",
    "WIN_LENGTH          = 1024\n",
    "N_FFT               = 1024\n",
    "N_MELS              = 80\n",
    "F_MIN               = 0.0\n",
    "F_MAX               = 8000.0\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 4. Training\n",
    "##############################################\n",
    "N_EPOCHS            = 100\n",
    "MAX_ITERATIONS      = 50000\n",
    "BATCH_SIZE          = 16\n",
    "LEARNING_RATE       = 0.000001\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 5. Model\n",
    "##############################################\n",
    "N_ENCODER_LAYERS    = 4\n",
    "N_HEADS             = 2\n",
    "HIDDEN_DIM          = 256\n",
    "KERNEL_SIZE         = 9\n",
    "DROPOUT             = 0.1\n",
    "ALPHA               = 0.1       # mixup\n",
    "BETA                = 1.0       # rank\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 6. Miscellaneous\n",
    "##############################################\n",
    "MARKER              = ['o', '^', 's', 'd']\n",
    "COLORS              = ['#7C00FE', '#F9E400', '#FFAF00', '#F5004F', '#00B2A9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b2adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping\n"
     ]
    }
   ],
   "source": [
    "# train, test 데이터셋 분리\n",
    "train_list, valid_list = [], []\n",
    "for speaker in SPEAKERS:\n",
    "    paths = glob.glob(os.path.join(PREPROCESSED_PATH, speaker, '*.npz'))\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    n_train = int(len(paths) * 0.8)\n",
    "    train_list.extend(paths[:n_train])\n",
    "    valid_list.extend(paths[n_train:])\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(PREPROCESSED_PATH, 'fs2_train.txt')):\n",
    "    # train, valid 데이터셋을 파일로 저장\n",
    "    with open(os.path.join(PREPROCESSED_PATH, 'fs2_train.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(train_list) + '\\n')\n",
    "\n",
    "    with open(os.path.join(PREPROCESSED_PATH, 'fs2_valid.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(valid_list) + '\\n')\n",
    "else:\n",
    "    print('Skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d165cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneme2sequence(phoneme):\n",
    "    seq = [VALID_TOKENS.index(token) for token in phoneme]\n",
    "    return seq\n",
    "\n",
    "def sequence2phoneme(sequence):\n",
    "    phoneme = [VALID_TOKENS[i] for i in sequence]\n",
    "    return phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa099617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517ecfbea3e74b60878dfe3854859f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melspectrogram shape: torch.Size([80, 305])\n",
      "Pitch shape: torch.Size([305])\n",
      "Energy shape: torch.Size([305])\n",
      "Duration shape: torch.Size([45])\n",
      "Phoneme sequence: torch.Size([45])\n",
      "*Total duration: 305\n",
      "Speaker index: tensor(0)\n",
      "Emotion index: tensor(4)\n",
      "Text: the promoter's eyes were heavy, with little puffy bags under them.\n"
     ]
    }
   ],
   "source": [
    "class FastSpeech2Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(FastSpeech2Dataset, self).__init__()\n",
    "        \n",
    "        self.data_paths = []\n",
    "        with open(os.path.join(PREPROCESSED_PATH, f'fs2_{mode}.txt'), 'r') as f:\n",
    "            self.data_paths = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        \n",
    "        # Load features\n",
    "        mel = data['mel']\n",
    "        pitch = data['pitch']\n",
    "        energy = data['energy']\n",
    "        duration = data['durations']\n",
    "        phoneme = data['phones'].tolist()\n",
    "\n",
    "        # metadata\n",
    "        speaker = data['speaker'].item()\n",
    "        emotion = data['emotion'].item()\n",
    "        text = data['transcript'].item().replace(NOISE_SYMBOL.strip(), '').strip()\n",
    "        audio_path = data['audio_path'].item()\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'mel': torch.FloatTensor(mel),\n",
    "            'pitch': torch.FloatTensor(pitch),\n",
    "            'energy': torch.FloatTensor(energy),\n",
    "            'duration': torch.LongTensor(duration),\n",
    "            'phoneme': torch.LongTensor(phoneme2sequence(phoneme)),\n",
    "            'speaker': torch.tensor(SPEAKERS.index(speaker), dtype=torch.long),\n",
    "            'emotion': torch.tensor(EMOTIONS.index(emotion), dtype=torch.long),\n",
    "            'text': text,\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = FastSpeech2Dataset(mode='train')\n",
    "for data in tqdm.notebook.tqdm(dataset):\n",
    "    print('Melspectrogram shape:', data['mel'].shape)\n",
    "    print('Pitch shape:', data['pitch'].shape)\n",
    "    print('Energy shape:', data['energy'].shape)\n",
    "    print('Duration shape:', data['duration'].shape)\n",
    "    print('Phoneme sequence:', data['phoneme'].shape)\n",
    "    print('*Total duration:', data['duration'].sum().item())\n",
    "    print('Speaker index:', data['speaker'])\n",
    "    print('Emotion index:', data['emotion'])\n",
    "    print('Text:', data['text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675be7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme shape: torch.Size([16, 58])\n",
      "Input lengths: tensor([3, 3, 0, 2, 3, 3, 3, 2, 3, 0, 3, 0, 0, 2, 3, 3])\n",
      "Mel shape: torch.Size([16])\n",
      "Pitch shape: torch.Size([16, 506, 80])\n",
      "Energy shape: torch.Size([16, 506])\n",
      "Duration shape: torch.Size([16, 506])\n",
      "Output lengths: tensor([[ 5,  4, 11, 13, 15,  5, 10,  5,  8, 26,  3,  7,  3,  7, 12, 25,  6, 12,\n",
      "          5,  3,  5,  7,  2,  7,  3,  3,  3,  4, 13,  2,  3,  3,  5,  2, 10, 14,\n",
      "         39,  4,  5,  4, 11, 10,  2,  9,  4,  8, 68,  7,  4,  2,  2,  6, 14, 13,\n",
      "         10,  6,  4,  3],\n",
      "        [ 2,  6,  3,  8,  5,  8,  5,  3,  3,  9,  7,  8, 19,  5,  5, 12, 18,  9,\n",
      "         13, 10,  2, 13, 23,  2,  3,  5,  3, 28,  5,  2, 34,  7, 14,  4, 12, 36,\n",
      "          7,  9,  6,  8,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 3,  2,  8,  3,  7,  6,  5,  2,  4,  5,  5, 11,  3, 13,  5,  9, 12,  5,\n",
      "          5,  2,  3,  2,  9,  2,  4,  5,  4,  4,  2,  3,  5,  4,  2,  2,  3,  4,\n",
      "          3,  3, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  9,  8,  2,  3,  5,  7,  5,  4,  7,  2,  5,  1,  2,  5,  9,  1,  9,\n",
      "          6,  5,  2,  5,  2,  4,  5,  5, 10,  3,  3,  7,  5,  2,  9,  2, 12,  2,\n",
      "         12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 3,  2,  4,  3,  4,  3,  5,  5,  3,  5,  2,  8,  3,  3,  9,  8,  2,  3,\n",
      "          6,  1,  3,  7,  8,  6,  4,  3,  6,  4,  3,  7, 10,  7,  4,  3,  7,  1,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 4,  2,  4,  3,  5,  2,  3,  9, 10,  6,  5,  7,  2,  3,  6,  9,  8,  2,\n",
      "          2,  5,  9,  4,  3,  9,  4,  4,  2,  3,  6,  6,  7,  2,  9, 13,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  6,  5,  5,  2,  2,  2,  3,  5,  7,  3,  5,  5,  3,  7,  7,  5,  1,\n",
      "         11,  8,  3,  7,  7,  3,  2,  4,  4,  5, 11,  7, 11,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  2,  3,  7,  3,  2,  9,  4,  5,  5,  2,  3,  4,  4,  3,  3,  2,  3,\n",
      "         10,  3,  1,  7,  3,  2,  3,  3,  6,  6, 10,  9,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [19,  4,  6, 13, 12,  6,  2,  3,  8, 24,  3, 13,  4,  4, 16, 22,  4,  6,\n",
      "         15,  5, 10,  1,  4,  6, 13,  9,  5,  5, 13, 14,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 8,  3,  4,  4,  8,  4,  5,  3,  4,  7,  3,  8,  3,  6,  3,  3,  8,  3,\n",
      "          4,  6,  2,  2,  6,  4,  5,  4,  5,  9,  7,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 5,  3,  7,  8, 10,  2,  4,  4,  4,  6,  3,  2, 13,  5,  4,  5,  2,  9,\n",
      "          8,  3, 16,  6,  6,  3, 11, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 7,  4,  4,  7,  5,  3,  3,  2,  3,  5,  6, 12,  4,  3,  2,  2,  3,  6,\n",
      "          9,  3, 15,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 3, 14,  5,  4,  5, 21,  8,  8,  7, 16,  8,  6,  3,  8,  2, 10,  9,  5,\n",
      "          2, 14,  4,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  8,  6,  5,  9,  5, 13,  9,  3,  7,  4,  2,  5,  3,  4,  8,  2,  3,\n",
      "          5,  7,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 9,  3,  8,  7,  2,  3,  3,  4,  9,  7,  2,  5,  5,  4,  9,  6,  6,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2, 10,  9,  4, 37, 25,  8,  2,  3,  5, 13, 14, 12, 12,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "Speakers: tensor([506, 388, 193, 187, 165, 178, 158, 129, 269, 141, 165, 113, 167, 114,\n",
      "         92, 156])\n",
      "Labels: ['this tacit promise of continued acquaintance gave saxon a little joy-thrill.', 'so we have to fit the boat throughout with oil lamps as well.', 'the fourth and fifth days passed without any developments.', 'her own betrayal of herself was like tonic to philip.', \"it was not red-eye's way to forego revenge so easily.\"]\n",
      "Wavs: ['/workspace/montreal_forced_aligner/corpus/sam/disgusted_0298.wav', '/workspace/montreal_forced_aligner/corpus/sam/amused_0410.wav', '/workspace/montreal_forced_aligner/corpus/bea/neutral_0064.wav', '/workspace/montreal_forced_aligner/corpus/josh/sleepy_0044.wav', '/workspace/montreal_forced_aligner/corpus/sam/angry_0324.wav']\n"
     ]
    }
   ],
   "source": [
    "class TextMelCollateWithAlignment:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # Right zero-pad all one-hot text sequences to the max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x['phoneme']) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "\n",
    "    \n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        phoneme_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        phoneme_padded.zero_()\n",
    "        duration_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        duration_padded.zero_()\n",
    "\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            phoneme = batch[ids_sorted_decreasing[i]]['phoneme']\n",
    "            phoneme_padded[i, :phoneme.size(0)] = phoneme\n",
    "            duration = batch[ids_sorted_decreasing[i]]['duration']\n",
    "            duration_padded[i, :duration.size(0)] = duration\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0]['mel'].size(0)\n",
    "        max_target_len = max([x['mel'].size(1) for x in batch])\n",
    "\n",
    "        # include mel padded and gate padded\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        pitch_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        pitch_padded.zero_()\n",
    "        energy_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        energy_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        labels, wavs = [], []\n",
    "        speakers = torch.LongTensor(len(batch))\n",
    "\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            idx = ids_sorted_decreasing[i]\n",
    "            mel = batch[idx]['mel']\n",
    "            pitch = batch[idx]['pitch']\n",
    "            energy = batch[idx]['energy']\n",
    "            mel_padded[i, :, :mel.size(1)] = mel\n",
    "            pitch_padded[i, :pitch.size(0)] = pitch\n",
    "            energy_padded[i, :energy.size(0)] = energy\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            labels.append(batch[idx]['text'])\n",
    "            wavs.append(batch[idx]['audio_path'])\n",
    "            speakers[i] = batch[idx]['speaker']\n",
    "\n",
    "        mel_padded = mel_padded.permute(0, 2, 1)\n",
    "        return (\n",
    "            phoneme_padded,\n",
    "            speakers,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            pitch_padded,\n",
    "            energy_padded,\n",
    "            duration_padded,\n",
    "            output_lengths,\n",
    "            labels,\n",
    "            wavs,\n",
    "        )\n",
    "\n",
    "dataset = FastSpeech2Dataset(mode='train')\n",
    "collate_fn = TextMelCollateWithAlignment()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    phoneme, speakers, input_lengths, mel, pitch, energy, duration, output_lengths, labels, wavs = batch\n",
    "    print('Phoneme shape:', phoneme.shape)\n",
    "    print('Input lengths:', input_lengths)\n",
    "    print('Mel shape:', mel.shape)\n",
    "    print('Pitch shape:', pitch.shape)\n",
    "    print('Energy shape:', energy.shape)\n",
    "    print('Duration shape:', duration.shape)\n",
    "    print('Output lengths:', output_lengths)\n",
    "    print('Speakers:', speakers)  # Print first 5 speaker indices\n",
    "    print('Labels:', labels[:5])  # Print first 5 labels\n",
    "    print('Wavs:', wavs[:5])      # Print first 5 audio paths\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6560bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to designated device\n",
    "def batch_to_device(batch, device):\n",
    "    return (\n",
    "        batch[0].to(device),  # phoneme\n",
    "        batch[1].to(device),  # speaker\n",
    "        batch[2].to(device),  # input_lengths\n",
    "        batch[3].to(device),  # mel\n",
    "        batch[4].to(device),  # pitch\n",
    "        batch[5].to(device),  # energy\n",
    "        batch[6].to(device),  # duration\n",
    "        batch[7].to(device),  # output_lengths\n",
    "        batch[8],             # labels (strings)\n",
    "        batch[9],             # wavs (file paths)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae233cb",
   "metadata": {},
   "source": [
    "### FastSpeech2 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908519be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural network modules for the FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\n",
    "synthesis model\n",
    "Authors\n",
    "* Sathvik Udupa 2022\n",
    "* Pradnya Kandarkar 2023\n",
    "* Yingzhi Wang 2023\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from speechbrain.lobes.models.transformer.Transformer import (\n",
    "    PositionalEncoding,\n",
    "    TransformerEncoder,\n",
    "    get_key_padding_mask,\n",
    "    get_mask_from_lengths,\n",
    ")\n",
    "from speechbrain.nnet import CNN, linear\n",
    "from speechbrain.nnet.embedding import Embedding\n",
    "from speechbrain.nnet.losses import bce_loss\n",
    "from speechbrain.nnet.normalization import LayerNorm\n",
    "\n",
    "from speechbrain.lobes.models.FastSpeech2 import (\n",
    "    EncoderPreNet,\n",
    "    DurationPredictor,\n",
    "    PostNet,\n",
    "    upsample,\n",
    "    average_over_durations,\n",
    "    SSIMLoss,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FastSpeech2(nn.Module):\n",
    "    \"\"\"The FastSpeech2 text-to-speech model.\n",
    "    This class is the main entry point for the model, which is responsible\n",
    "    for instantiating all submodules, which, in turn, manage the individual\n",
    "    neural network layers\n",
    "    Simplified STRUCTURE: input->token embedding ->encoder ->duration/pitch/energy predictor ->duration\n",
    "    upsampler -> decoder -> output\n",
    "    During training, teacher forcing is used (ground truth durations are used for upsampling)\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    enc_num_layers: int\n",
    "        number of transformer layers (TransformerEncoderLayer) in encoder\n",
    "    enc_num_head: int\n",
    "        number of multi-head-attention (MHA) heads in encoder transformer layers\n",
    "    enc_d_model: int\n",
    "        the number of expected features in the encoder\n",
    "    enc_ffn_dim: int\n",
    "        the dimension of the feedforward network model\n",
    "    enc_k_dim: int\n",
    "        the dimension of the key\n",
    "    enc_v_dim: int\n",
    "        the dimension of the value\n",
    "    enc_dropout: float\n",
    "        Dropout for the encoder\n",
    "    dec_num_layers: int\n",
    "        number of transformer layers (TransformerEncoderLayer) in decoder\n",
    "    dec_num_head: int\n",
    "        number of multi-head-attention (MHA) heads in decoder transformer layers\n",
    "    dec_d_model: int\n",
    "        the number of expected features in the decoder\n",
    "    dec_ffn_dim: int\n",
    "        the dimension of the feedforward network model\n",
    "    dec_k_dim: int\n",
    "        the dimension of the key\n",
    "    dec_v_dim: int\n",
    "        the dimension of the value\n",
    "    dec_dropout: float\n",
    "        dropout for the decoder\n",
    "    normalize_before: bool\n",
    "        whether normalization should be applied before or after MHA or FFN in Transformer layers.\n",
    "    ffn_type: str\n",
    "        whether to use convolutional layers instead of feed forward network inside transformer layer.\n",
    "    ffn_cnn_kernel_size_list: list of int\n",
    "        conv kernel size of 2 1d-convs if ffn_type is 1dcnn\n",
    "    n_char: int\n",
    "        the number of symbols for the token embedding\n",
    "    n_mels: int\n",
    "        number of bins in mel spectrogram\n",
    "    postnet_embedding_dim: int\n",
    "       output feature dimension for convolution layers\n",
    "    postnet_kernel_size: int\n",
    "       postnet convolution kernel size\n",
    "    postnet_n_convolutions: int\n",
    "       number of convolution layers\n",
    "    postnet_dropout: float\n",
    "        dropout probability for postnet\n",
    "    padding_idx: int\n",
    "        the index for padding\n",
    "    dur_pred_kernel_size: int\n",
    "        the convolution kernel size in duration predictor\n",
    "    pitch_pred_kernel_size: int\n",
    "        kernel size for pitch prediction.\n",
    "    energy_pred_kernel_size: int\n",
    "        kernel size for energy prediction.\n",
    "    variance_predictor_dropout: float\n",
    "        dropout probability for variance predictor (duration/pitch/energy)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.FastSpeech2 import FastSpeech2\n",
    "    >>> model = FastSpeech2(\n",
    "    ...    enc_num_layers=6,\n",
    "    ...    enc_num_head=2,\n",
    "    ...    enc_d_model=384,\n",
    "    ...    enc_ffn_dim=1536,\n",
    "    ...    enc_k_dim=384,\n",
    "    ...    enc_v_dim=384,\n",
    "    ...    enc_dropout=0.1,\n",
    "    ...    dec_num_layers=6,\n",
    "    ...    dec_num_head=2,\n",
    "    ...    dec_d_model=384,\n",
    "    ...    dec_ffn_dim=1536,\n",
    "    ...    dec_k_dim=384,\n",
    "    ...    dec_v_dim=384,\n",
    "    ...    dec_dropout=0.1,\n",
    "    ...    normalize_before=False,\n",
    "    ...    ffn_type='1dcnn',\n",
    "    ...    ffn_cnn_kernel_size_list=[9, 1],\n",
    "    ...    n_char=40,\n",
    "    ...    n_mels=80,\n",
    "    ...    postnet_embedding_dim=512,\n",
    "    ...    postnet_kernel_size=5,\n",
    "    ...    postnet_n_convolutions=5,\n",
    "    ...    postnet_dropout=0.5,\n",
    "    ...    padding_idx=0,\n",
    "    ...    dur_pred_kernel_size=3,\n",
    "    ...    pitch_pred_kernel_size=3,\n",
    "    ...    energy_pred_kernel_size=3,\n",
    "    ...    variance_predictor_dropout=0.5)\n",
    "    >>> inputs = torch.tensor([\n",
    "    ...     [13, 12, 31, 14, 19],\n",
    "    ...     [31, 16, 30, 31, 0],\n",
    "    ... ])\n",
    "    >>> input_lengths = torch.tensor([5, 4])\n",
    "    >>> durations = torch.tensor([\n",
    "    ...     [2, 4, 1, 5, 3],\n",
    "    ...     [1, 2, 4, 3, 0],\n",
    "    ... ])\n",
    "    >>> mel_post, postnet_output, predict_durations, predict_pitch, avg_pitch, predict_energy, avg_energy, mel_lens = model(inputs, durations=durations)\n",
    "    >>> mel_post.shape, predict_durations.shape\n",
    "    (torch.Size([2, 15, 80]), torch.Size([2, 5]))\n",
    "    >>> predict_pitch.shape, predict_energy.shape\n",
    "    (torch.Size([2, 5, 1]), torch.Size([2, 5, 1]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # encoder parameters\n",
    "        enc_num_layers,\n",
    "        enc_num_head,\n",
    "        enc_d_model,\n",
    "        enc_ffn_dim,\n",
    "        enc_k_dim,\n",
    "        enc_v_dim,\n",
    "        enc_dropout,\n",
    "        # decoder parameters\n",
    "        dec_num_layers,\n",
    "        dec_num_head,\n",
    "        dec_d_model,\n",
    "        dec_ffn_dim,\n",
    "        dec_k_dim,\n",
    "        dec_v_dim,\n",
    "        dec_dropout,\n",
    "        normalize_before,\n",
    "        ffn_type,\n",
    "        ffn_cnn_kernel_size_list,\n",
    "        n_char,\n",
    "        n_mels,\n",
    "        postnet_embedding_dim,\n",
    "        postnet_kernel_size,\n",
    "        postnet_n_convolutions,\n",
    "        postnet_dropout,\n",
    "        padding_idx,\n",
    "        dur_pred_kernel_size,\n",
    "        pitch_pred_kernel_size,\n",
    "        energy_pred_kernel_size,\n",
    "        variance_predictor_dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc_num_head = enc_num_head\n",
    "        self.dec_num_head = dec_num_head\n",
    "        self.padding_idx = padding_idx\n",
    "        self.sinusoidal_positional_embed_encoder = PositionalEncoding(\n",
    "            enc_d_model\n",
    "        )\n",
    "        self.sinusoidal_positional_embed_decoder = PositionalEncoding(\n",
    "            dec_d_model\n",
    "        )\n",
    "\n",
    "        self.speaker_emb = Embedding(\n",
    "            num_embeddings=len(SPEAKERS),\n",
    "            embedding_dim=enc_d_model,\n",
    "            # padding_idx=padding_idx,\n",
    "        )\n",
    "        self.encPreNet = EncoderPreNet(\n",
    "            n_char, padding_idx, out_channels=enc_d_model\n",
    "        )\n",
    "        self.durPred = DurationPredictor(\n",
    "            in_channels=enc_d_model,\n",
    "            out_channels=enc_d_model,\n",
    "            kernel_size=dur_pred_kernel_size,\n",
    "            dropout=variance_predictor_dropout,\n",
    "        )\n",
    "        self.pitchPred = DurationPredictor(\n",
    "            in_channels=enc_d_model,\n",
    "            out_channels=enc_d_model,\n",
    "            kernel_size=dur_pred_kernel_size,\n",
    "            dropout=variance_predictor_dropout,\n",
    "        )\n",
    "        self.energyPred = DurationPredictor(\n",
    "            in_channels=enc_d_model,\n",
    "            out_channels=enc_d_model,\n",
    "            kernel_size=dur_pred_kernel_size,\n",
    "            dropout=variance_predictor_dropout,\n",
    "        )\n",
    "        self.pitchEmbed = CNN.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=enc_d_model,\n",
    "            kernel_size=pitch_pred_kernel_size,\n",
    "            padding=\"same\",\n",
    "            skip_transpose=True,\n",
    "        )\n",
    "\n",
    "        self.energyEmbed = CNN.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=enc_d_model,\n",
    "            kernel_size=energy_pred_kernel_size,\n",
    "            padding=\"same\",\n",
    "            skip_transpose=True,\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=enc_num_layers,\n",
    "            nhead=enc_num_head,\n",
    "            d_ffn=enc_ffn_dim,\n",
    "            d_model=enc_d_model,\n",
    "            kdim=enc_k_dim,\n",
    "            vdim=enc_v_dim,\n",
    "            dropout=enc_dropout,\n",
    "            activation=nn.ReLU,\n",
    "            normalize_before=normalize_before,\n",
    "            ffn_type=ffn_type,\n",
    "            ffn_cnn_kernel_size_list=ffn_cnn_kernel_size_list,\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerEncoder(\n",
    "            num_layers=dec_num_layers,\n",
    "            nhead=dec_num_head,\n",
    "            d_ffn=dec_ffn_dim,\n",
    "            d_model=dec_d_model,\n",
    "            kdim=dec_k_dim,\n",
    "            vdim=dec_v_dim,\n",
    "            dropout=dec_dropout,\n",
    "            activation=nn.ReLU,\n",
    "            normalize_before=normalize_before,\n",
    "            ffn_type=ffn_type,\n",
    "            ffn_cnn_kernel_size_list=ffn_cnn_kernel_size_list,\n",
    "        )\n",
    "\n",
    "        self.linear = linear.Linear(n_neurons=n_mels, input_size=dec_d_model)\n",
    "        self.postnet = PostNet(\n",
    "            n_mel_channels=n_mels,\n",
    "            postnet_embedding_dim=postnet_embedding_dim,\n",
    "            postnet_kernel_size=postnet_kernel_size,\n",
    "            postnet_n_convolutions=postnet_n_convolutions,\n",
    "            postnet_dropout=postnet_dropout,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens,\n",
    "        speakers,\n",
    "        durations=None,\n",
    "        pitch=None,\n",
    "        energy=None,\n",
    "        pace=1.0,\n",
    "        pitch_rate=1.0,\n",
    "        energy_rate=1.0,\n",
    "    ):\n",
    "        \"\"\"forward pass for training and inference\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        tokens: torch.Tensor\n",
    "            batch of input tokens\n",
    "        durations: torch.Tensor\n",
    "            batch of durations for each token. If it is None, the model will infer on predicted durations\n",
    "        pitch: torch.Tensor\n",
    "            batch of pitch for each frame. If it is None, the model will infer on predicted pitches\n",
    "        energy: torch.Tensor\n",
    "            batch of energy for each frame. If it is None, the model will infer on predicted energies\n",
    "        pace: float\n",
    "            scaling factor for durations\n",
    "        pitch_rate: float\n",
    "            scaling factor for pitches\n",
    "        energy_rate: float\n",
    "            scaling factor for energies\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_post: torch.Tensor\n",
    "            mel outputs from the decoder\n",
    "        postnet_output: torch.Tensor\n",
    "            mel outputs from the postnet\n",
    "        predict_durations: torch.Tensor\n",
    "            predicted durations of each token\n",
    "        predict_pitch: torch.Tensor\n",
    "            predicted pitches of each token\n",
    "        avg_pitch: torch.Tensor\n",
    "            target pitches for each token if input pitch is not None\n",
    "            None if input pitch is None\n",
    "        predict_energy: torch.Tensor\n",
    "            predicted energies of each token\n",
    "        avg_energy: torch.Tensor\n",
    "            target energies for each token if input energy is not None\n",
    "            None if input energy is None\n",
    "        mel_length:\n",
    "            predicted lengths of mel spectrograms\n",
    "        \"\"\"\n",
    "        srcmask = get_key_padding_mask(tokens, pad_idx=self.padding_idx)\n",
    "        srcmask_inverted = (~srcmask).unsqueeze(-1)\n",
    "\n",
    "        # prenet & encoder\n",
    "        token_feats = self.encPreNet(tokens)\n",
    "        pos = self.sinusoidal_positional_embed_encoder(token_feats)\n",
    "        token_feats = torch.add(token_feats, pos) * srcmask_inverted\n",
    "        attn_mask = (\n",
    "            srcmask.unsqueeze(-1)\n",
    "            .repeat(self.enc_num_head, 1, token_feats.shape[1])\n",
    "            .permute(0, 2, 1)\n",
    "            .bool()\n",
    "        )\n",
    "        token_feats, _ = self.encoder(\n",
    "            token_feats, src_mask=attn_mask, src_key_padding_mask=srcmask\n",
    "        )\n",
    "        token_feats = token_feats * srcmask_inverted\n",
    "\n",
    "        # ADD SPEAKER EMBEDDING -- modification.\n",
    "        token_feats = token_feats + self.speaker_emb(speakers).unsqueeze(1).expand(\n",
    "            -1, token_feats.shape[1], -1\n",
    "        )\n",
    "\n",
    "        # duration predictor\n",
    "        predict_durations = self.durPred(token_feats, srcmask_inverted).squeeze(\n",
    "            -1\n",
    "        )\n",
    "\n",
    "        if predict_durations.dim() == 1:\n",
    "            predict_durations = predict_durations.unsqueeze(0)\n",
    "        if durations is None:\n",
    "            dur_pred_reverse_log = torch.clamp(\n",
    "                torch.special.expm1(predict_durations), 0\n",
    "            )\n",
    "\n",
    "        # pitch predictor\n",
    "        avg_pitch = None\n",
    "        predict_pitch = self.pitchPred(token_feats, srcmask_inverted)\n",
    "        # use a pitch rate to adjust the pitch\n",
    "        predict_pitch = predict_pitch * pitch_rate\n",
    "        if pitch is not None:\n",
    "            avg_pitch = average_over_durations(pitch.unsqueeze(1), durations)\n",
    "            pitch = self.pitchEmbed(avg_pitch)\n",
    "            avg_pitch = avg_pitch.permute(0, 2, 1)\n",
    "        else:\n",
    "            pitch = self.pitchEmbed(predict_pitch.permute(0, 2, 1))\n",
    "        pitch = pitch.permute(0, 2, 1)\n",
    "        token_feats = token_feats.add(pitch)\n",
    "\n",
    "        # energy predictor\n",
    "        avg_energy = None\n",
    "        predict_energy = self.energyPred(token_feats, srcmask_inverted)\n",
    "        # use an energy rate to adjust the energy\n",
    "        predict_energy = predict_energy * energy_rate\n",
    "        if energy is not None:\n",
    "            avg_energy = average_over_durations(energy.unsqueeze(1), durations)\n",
    "            energy = self.energyEmbed(avg_energy)\n",
    "            avg_energy = avg_energy.permute(0, 2, 1)\n",
    "        else:\n",
    "            energy = self.energyEmbed(predict_energy.permute(0, 2, 1))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        token_feats = token_feats.add(energy)\n",
    "\n",
    "        # upsamples the durations\n",
    "        spec_feats, mel_lens = upsample(\n",
    "            token_feats,\n",
    "            durations if durations is not None else dur_pred_reverse_log,\n",
    "            pace=pace,\n",
    "        )\n",
    "        srcmask = get_mask_from_lengths(torch.tensor(mel_lens))\n",
    "        srcmask = srcmask.to(spec_feats.device)\n",
    "        srcmask_inverted = (~srcmask).unsqueeze(-1)\n",
    "        attn_mask = (\n",
    "            srcmask.unsqueeze(-1)\n",
    "            .repeat(self.dec_num_head, 1, spec_feats.shape[1])\n",
    "            .permute(0, 2, 1)\n",
    "            .bool()\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        pos = self.sinusoidal_positional_embed_decoder(spec_feats)\n",
    "        spec_feats = torch.add(spec_feats, pos) * srcmask_inverted\n",
    "\n",
    "        output_mel_feats, memory, *_ = self.decoder(\n",
    "            spec_feats, src_mask=attn_mask, src_key_padding_mask=srcmask\n",
    "        )\n",
    "\n",
    "        # postnet\n",
    "        mel_post = self.linear(output_mel_feats) * srcmask_inverted\n",
    "        postnet_output = self.postnet(mel_post) + mel_post\n",
    "        return (\n",
    "            mel_post,\n",
    "            postnet_output,\n",
    "            predict_durations,\n",
    "            predict_pitch,\n",
    "            avg_pitch,\n",
    "            predict_energy,\n",
    "            avg_energy,\n",
    "            torch.tensor(mel_lens),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    \"\"\"Loss Computation\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    log_scale_durations: bool\n",
    "        applies logarithm to target durations\n",
    "    ssim_loss_weight: float\n",
    "        weight for ssim loss\n",
    "    duration_loss_weight: float\n",
    "        weight for the duration loss\n",
    "    pitch_loss_weight: float\n",
    "        weight for the pitch loss\n",
    "    energy_loss_weight: float\n",
    "        weight for the energy loss\n",
    "    mel_loss_weight: float\n",
    "        weight for the mel loss\n",
    "    postnet_mel_loss_weight: float\n",
    "        weight for the postnet mel loss\n",
    "    spn_loss_weight: float\n",
    "        weight for spn loss\n",
    "    spn_loss_max_epochs: int\n",
    "        Max number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_scale_durations,\n",
    "        ssim_loss_weight,\n",
    "        duration_loss_weight,\n",
    "        pitch_loss_weight,\n",
    "        energy_loss_weight,\n",
    "        mel_loss_weight,\n",
    "        postnet_mel_loss_weight,\n",
    "        spn_loss_weight=1.0,\n",
    "        spn_loss_max_epochs=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ssim_loss = SSIMLoss()\n",
    "        self.mel_loss = nn.MSELoss()\n",
    "        self.postnet_mel_loss = nn.MSELoss()\n",
    "        self.dur_loss = nn.MSELoss()\n",
    "        self.pitch_loss = nn.MSELoss()\n",
    "        self.energy_loss = nn.MSELoss()\n",
    "        self.log_scale_durations = log_scale_durations\n",
    "        self.ssim_loss_weight = ssim_loss_weight\n",
    "        self.mel_loss_weight = mel_loss_weight\n",
    "        self.postnet_mel_loss_weight = postnet_mel_loss_weight\n",
    "        self.duration_loss_weight = duration_loss_weight\n",
    "        self.pitch_loss_weight = pitch_loss_weight\n",
    "        self.energy_loss_weight = energy_loss_weight\n",
    "        self.spn_loss_weight = spn_loss_weight\n",
    "        self.spn_loss_max_epochs = spn_loss_max_epochs\n",
    "\n",
    "\n",
    "    def forward(self, predictions, targets, current_epoch):\n",
    "        \"\"\"Computes the value of the loss function and updates stats\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions: tuple\n",
    "            model predictions\n",
    "        targets: tuple\n",
    "            ground truth data\n",
    "        current_epoch: int\n",
    "            The count of the current epoch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: torch.Tensor\n",
    "            the loss value\n",
    "        \"\"\"\n",
    "        (\n",
    "            mel_target,\n",
    "            target_durations,\n",
    "            target_pitch,\n",
    "            target_energy,\n",
    "            mel_length,\n",
    "            phon_len,\n",
    "            # spn_labels,\n",
    "        ) = targets\n",
    "        assert len(mel_target.shape) == 3\n",
    "        (\n",
    "            mel_out,\n",
    "            postnet_mel_out,\n",
    "            log_durations,\n",
    "            predicted_pitch,\n",
    "            average_pitch,\n",
    "            predicted_energy,\n",
    "            average_energy,\n",
    "            mel_lens,\n",
    "            # spn_preds,\n",
    "        ) = predictions\n",
    "\n",
    "        predicted_pitch = predicted_pitch.squeeze(-1)\n",
    "        predicted_energy = predicted_energy.squeeze(-1)\n",
    "\n",
    "        target_pitch = average_pitch.squeeze(-1)\n",
    "        target_energy = average_energy.squeeze(-1)\n",
    "\n",
    "        log_durations = log_durations.squeeze(-1)\n",
    "        if self.log_scale_durations:\n",
    "            log_target_durations = torch.log1p(target_durations.float())\n",
    "        # change this to perform batch level using padding mask\n",
    "\n",
    "        for i in range(mel_target.shape[0]):\n",
    "            if i == 0:\n",
    "                mel_loss = self.mel_loss(\n",
    "                    mel_out[i, : mel_length[i], :],\n",
    "                    mel_target[i, : mel_length[i], :],\n",
    "                )\n",
    "                postnet_mel_loss = self.postnet_mel_loss(\n",
    "                    postnet_mel_out[i, : mel_length[i], :],\n",
    "                    mel_target[i, : mel_length[i], :],\n",
    "                )\n",
    "                dur_loss = self.dur_loss(\n",
    "                    log_durations[i, : phon_len[i]],\n",
    "                    log_target_durations[i, : phon_len[i]].to(torch.float32),\n",
    "                )\n",
    "                pitch_loss = self.pitch_loss(\n",
    "                    predicted_pitch[i, : mel_length[i]],\n",
    "                    target_pitch[i, : mel_length[i]].to(torch.float32),\n",
    "                )\n",
    "                energy_loss = self.energy_loss(\n",
    "                    predicted_energy[i, : mel_length[i]],\n",
    "                    target_energy[i, : mel_length[i]].to(torch.float32),\n",
    "                )\n",
    "            else:\n",
    "                mel_loss = mel_loss + self.mel_loss(\n",
    "                    mel_out[i, : mel_length[i], :],\n",
    "                    mel_target[i, : mel_length[i], :],\n",
    "                )\n",
    "                postnet_mel_loss = postnet_mel_loss + self.postnet_mel_loss(\n",
    "                    postnet_mel_out[i, : mel_length[i], :],\n",
    "                    mel_target[i, : mel_length[i], :],\n",
    "                )\n",
    "                dur_loss = dur_loss + self.dur_loss(\n",
    "                    log_durations[i, : phon_len[i]],\n",
    "                    log_target_durations[i, : phon_len[i]].to(torch.float32),\n",
    "                )\n",
    "                pitch_loss = pitch_loss + self.pitch_loss(\n",
    "                    predicted_pitch[i, : mel_length[i]],\n",
    "                    target_pitch[i, : mel_length[i]].to(torch.float32),\n",
    "                )\n",
    "                energy_loss = energy_loss + self.energy_loss(\n",
    "                    predicted_energy[i, : mel_length[i]],\n",
    "                    target_energy[i, : mel_length[i]].to(torch.float32),\n",
    "                )\n",
    "        ssim_loss = self.ssim_loss(mel_out, mel_target, mel_length)\n",
    "        mel_loss = torch.div(mel_loss, len(mel_target))\n",
    "        postnet_mel_loss = torch.div(postnet_mel_loss, len(mel_target))\n",
    "        dur_loss = torch.div(dur_loss, len(mel_target))\n",
    "        pitch_loss = torch.div(pitch_loss, len(mel_target))\n",
    "        energy_loss = torch.div(energy_loss, len(mel_target))\n",
    "\n",
    "        # spn_loss = bce_loss(spn_preds, spn_labels)\n",
    "        # if current_epoch > self.spn_loss_max_epochs:\n",
    "        #     self.spn_loss_weight = 0\n",
    "\n",
    "        total_loss = (\n",
    "            ssim_loss * self.ssim_loss_weight\n",
    "            + mel_loss * self.mel_loss_weight\n",
    "            + postnet_mel_loss * self.postnet_mel_loss_weight\n",
    "            + dur_loss * self.duration_loss_weight\n",
    "            + pitch_loss * self.pitch_loss_weight\n",
    "            + energy_loss * self.energy_loss_weight\n",
    "            # + spn_loss * self.spn_loss_weight\n",
    "        )\n",
    "\n",
    "        loss = {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"ssim_loss\": ssim_loss * self.ssim_loss_weight,\n",
    "            \"mel_loss\": mel_loss * self.mel_loss_weight,\n",
    "            \"postnet_mel_loss\": postnet_mel_loss * self.postnet_mel_loss_weight,\n",
    "            \"dur_loss\": dur_loss * self.duration_loss_weight,\n",
    "            \"pitch_loss\": pitch_loss * self.pitch_loss_weight,\n",
    "            \"energy_loss\": energy_loss * self.energy_loss_weight,\n",
    "            # \"spn_loss\": spn_loss * self.spn_loss_weight,\n",
    "        }\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffaa7a",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735a13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = os.path.join('/workspace/emo_rank_tts/params.yaml')\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "# misc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dataset\n",
    "dataset = FastSpeech2Dataset(mode='train')\n",
    "collate_fn = TextMelCollateWithAlignment()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# model\n",
    "model = FastSpeech2(**config['fastspeech2']['model']).to(device)\n",
    "\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# loss\n",
    "criterion = Loss(**config['fastspeech2']['loss']).to(device)\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, 400):\n",
    "\n",
    "    epoch_avg_loss = defaultdict(float)\n",
    "\n",
    "    for idx, batch in enumerate(tqdm.notebook.tqdm(dataloader)):\n",
    "        batch = batch_to_device(batch, device)\n",
    "        phoneme, speakers, phon_len, mel_target, target_pitch, target_energy, target_duration, mel_length, labels, wavs = batch\n",
    "        global_step += 1\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(phoneme, speakers, target_duration, target_pitch, target_energy)\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        targets = (mel_target, target_duration, target_pitch, target_energy, mel_length, phon_len)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss = criterion(predictions, targets, epoch)\n",
    "        loss['total_loss'].backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        for loss_name, loss_value in loss.items():\n",
    "            epoch_avg_loss[loss_name] += loss_value\n",
    "\n",
    "        # Print predicted mels\n",
    "        if idx == 0:\n",
    "            melspecs = predictions[0].cpu().detach().numpy()\n",
    "            y_melspecs = mel_target.cpu().detach().numpy()\n",
    "            all_melspecs = np.concatenate((melspecs, y_melspecs), axis=0)\n",
    "            fig, axes = plt.subplots(4, 4, figsize=(16, 10))\n",
    "            for ax_idx, (ax, mel) in enumerate(zip(axes.flatten(), all_melspecs)):\n",
    "                ax.imshow(mel.T, aspect='auto', origin='lower', interpolation='none')\n",
    "                if ax_idx < len(melspecs):\n",
    "                    label = f\"Pred {ax_idx + 1}\"\n",
    "                    color = 'blue'\n",
    "                else:\n",
    "                    label = f\"GT {ax_idx - len(melspecs) + 1}\"\n",
    "                    color = 'red'\n",
    "\n",
    "                ax.text(\n",
    "                    0.95, 0.95, label,\n",
    "                    horizontalalignment='right',\n",
    "                    verticalalignment='top',\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=12,\n",
    "                    fontweight='bold',\n",
    "                    color=color,\n",
    "                )\n",
    "                # ax.set_title('Generated Mel-Spectrogram')\n",
    "                # ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('epoch_{}.png'.format(epoch))\n",
    "            plt.close()\n",
    "\n",
    "        # end of epoch\n",
    "    \n",
    "    epoch_avg_loss = {k: v / len(dataloader) for k, v in epoch_avg_loss.items()}\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    print(\"=\" * 50)\n",
    "    for loss_name, loss_value in epoch_avg_loss.items():\n",
    "        print(\"{:<30s}{:>20.4f}\".format(loss_name, loss_value))\n",
    "    print(\"=\" * 50, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5ceec",
   "metadata": {},
   "source": [
    "## Experiment: Intensity extractor test - `2025-06-05`\n",
    "\n",
    "- <span style=\"color:red\">fix #1</span>: Rank model의 intensity extractor 추정 **Done**\n",
    "- <span style=\"color:red\">fix #2</span>: Rank loss를 Rank model의 외부의 별도 class로 지정 **Done**\n",
    "- <span style=\"color:red\">fix #3</span>: Rank model의 output 수정: `H_i, H_j, h_i, h_j, r_i, r_j` 의 값을 반환하도록 설정 **Done**\n",
    "- <span style=\"color:red\">fix #4</span>: Rank model 별도의 intensity extractor class 생성 **Done**\n",
    "\n",
    "---\n",
    "\n",
    "- <span style=\"color:blue\">imp #1</span>: intensity extractor의 output $\\mathbf{I}$ 에 대해 phoneme-wise하게 평균을 취하여 크기 변경: [$B$, $T_{mel}$, $H$] $\\rightarrow$ [$B$, $T_{phone}$, $H$]\n",
    "- <span style=\"color:blue\">imp #2</span>: Speaker ID를 사용할 것인지 speaker embedding을 사용할 것인지 실험을 통해 도출 (논문에서는 speaker id)\n",
    "- <span style=\"color:blue\">imp #3</span>: `phoneme_encoder_output`과 `intensity_representation`, `speaker_id` 를 concat하여 variance adaptor의 입력으로 feed.\n",
    "- <span style=\"color:blue\">imp #4</span>: 추론시에는 intensity_representation을 명시적으로 구할 수 없어, manual label을 사용 -> manual label을 구하기 위한 clustering 필요 ($N$-level averaging)\n",
    "\n",
    "---\n",
    "\n",
    "- Rank model 재학습 후, train dataset에 대한 intensity score 추출\n",
    "- Intensity score을 $N$ 개로 bucketize (min - median - max).\n",
    "- Speaker 별, emotion 별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d18099",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = torch.load('/workspace/experiments/exp_3/best_model.pth')\n",
    "intensity_extractor = rankm_model.intensity_extractor.to(device)\n",
    "\n",
    "# imp#1\n",
    "# -- textgrid? -> trimming 된 것 어떻게 처리할 것?\n",
    "# intensity extractor의 결과 I의 time dimension이 phoneme sequence duration과 일치\n",
    "# 각 phoneme sequence duration에 대해 I의 평균을 구한다.\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "averaged_intensity = []\n",
    "for d in duration:\n",
    "    phoneme_averaged = intensity[start_idx:start_idx + d].mean(dim=0)\n",
    "    averaged_intensity.append(phoneme_averaged)\n",
    "    start_idx += d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fine_grained_emo_tts -> train dataset에서 manual intensity 추출출\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc3e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.load('/workspace/emo_rank_tts/rank_model/result.npz', allow_pickle=True)['intensity'].item()['bea']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
